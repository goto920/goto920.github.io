<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
</head>
<body>
<h1>Capture desktop audio and video</h1>

<h2>localVideo (audio muted) (click speaker icon to unmute)</h2>

<video id="localVideoAndAudio" width="640" 
 autoplay controls playsinline muted></video><br/>
<button id="start" onclick = "openCapture()">Start</button>

<hr/>
<h2>Debug Messages</h2>
<p id="userAgent"></p>
<hr/>
<p id="message">Message: &nbsp;</p>
<hr/>

<script>

let localVideoAndAudio = document.getElementById("localVideoAndAudio");
let userAgent = document.getElementById("userAgent");
let message = document.getElementById("message");
let startButton = document.getElementById("start");

userAgent.innerHTML = "UserAgent: " + navigator.userAgent;

if (check()) {
  constraints = {video: true,  audio: true};
}

function check(){
  if (navigator.mediaDevices.getDisplayMedia &&
      typeof navigator.mediaDevices.getDisplayMedia === 'function'){
      message.innerHTML += "getDisplayMedia is supported.<br/>";
      return true;
  } else {
      message.innerHTML += "getDisplayMedia is NOT supported.<br/>";
      return false; 
  }
}

async function openCapture(constraints){
  try {
    let stream = await navigator.mediaDevices.getDisplayMedia(constraints);
    let tracks = stream.getAudioTracks();
    console.log('Audio tracks', tracks.length);

    if (tracks.length === 0){
      try {
         let stream2 = await navigator.mediaDevices.getUserMedia(
         { video: false,
         audio: {autoGainControl: false, echoCancellation: false,
         noiseSupppression: false}}
         );
         let audio = stream2.getAudioTracks();
         console.log('2nd stream audio: ' + audio.length);
         stream.addTrack(audio[0]);
      } catch(err) { alert(err); }
    }

    localVideoAndAudio.srcObject = stream;

  } catch (err) {
    alert(err);
  }
}

</script>
<script src="https://webrtc.github.io/adapter/adapter-latest.js"></script>
</body>
</html>
